---
title: "Apple Vision Pro and the Era of Humane Computing"
description: "A New Dawn in Technology: Apple Vision Pro Ushers in the Era of Humane Computing"
date: 2023-07-01
showTableOfContents: true
---

Apple announced the Vision Pro, its inaugural XR headset, at WWDC 2023. The Vision Pro is a mixed-reality device which aspires to harmoniously marry digital content with physical surroundings. User interaction is achieved through a combination of eye-tracking, hand-tracking, and dictation. The headset boasts dual micro-OLED displays, each offering over 4k resolution, enveloping spatial audio, dual-chips (M2 and the new R1 for signal processing), and the new visionOS which integrates with Apple's ecosystem and can run most iOS and iPadOS apps as well as mirror a Mac screen. It's scheduled to launch next year at a starting price of $3500.

As one might expect, the internet is buzzing with reactions to this announcement. Some are positive. Some are negative. Some are balanced. Some are biased. But what strikes me is that even the most astute analyses seems to be missing what I believe is the biggest shift that this technology could represent and which should excite anyone who spends a significant amount of their life using a computing device.

> ["[…] in the same way that Mac introduced us to personal computing, and iPhone introduced us to mobile computing, Apple Vision Pro will introduce us to spatial computing. This marks the beginning of a journey that will bring a new dimension to powerful personal technology."](https://www.youtube.com/live/GYkq9Rgoj8E?feature=share&t=5028) --- Tim Cook, WWDC23

Apple calls this "spatial computing". It's a fitting name, but it doesn't capture the full potential. I want to introduce a new term: "humane computing".

### What is Humane Computing?

The technology that we have integrated into our lives is often a double-edged sword. The convenience and empowerment it provides can come at the expense of our health. The toll of prolonged usage is often high and not visible until significant interest has accrued.

To define "humane," dictionaries often refer to traits such as compassion, benevolence, and the minimization of pain. I will define "humane computing" as a form of computing that either mitigates or eliminates the detriments associated with prolonged usage. Humane computing, as an ideal, asks for technology to be built in tandem with our human attributes---requiring adaptation from, and not to, technology.

When I refer to "computing," my focus is on tasks that demand intellectual engagement and creativity. Examples include coding, crafting written content, conducting research, graphic designing, and editing videos or images. These pursuits necessitate intricate interactions between the user and the computer, often over extended timeframes. In contrast, activities that are predominantly about consumption, such as casually scrolling through TikTok, Reddit, or the like, and occasionally leaving comments, don't fall under the umbrella of "computing" in the context of this discussion.

## The Price of Computing

### Move More

We are [built to move](https://thereadystate.com/built-to-move/).

But we don't move enough. We sit too much. We sit at our desks. We sit at our tables. We sit on our couches. We sit in our cars or busses or trains or planes. We sit on our toilets.
We've become excessively stationary. It seems like our days are an endless loop of finding the next seat. 

Some of us attempt to break free by embracing standing desks. But standing still is not much better than sitting still. Our bodies crave variety and movement: sitting cross-legged, squatting, kneeling, or even lying down, and frequent transitions.

If you use a computer regularly, you almost certainly sit with your body bent into two 90° angles (on a chair bent at your hips and knees) or stand stationary for long periods of time. You probably don't sit cross-legged. Or squat. Or kneel. Or lay on your back or stomach or side. Or actively move any part of your body other than your fingers and wrists. This is inhumane. It is inhumane to force our bodies into these stationary positions for long periods of time, because we are built for the opposite.

The problem is that we have stationary visual output (screens) and stationary user input (keyboards, mice, trackpads, etc.). To make matters worse, we often have to choose between physically linking the visual output with the user input (e.g. a laptop) or having virtually zero mobility by using separate and bulky input and output devices (e.g. a desktop with a monitor, keyboard, and mouse). All of these issues are due to physical constraints. Laptops screens are small not because small screens are desirable but because mobility is. Desktop monitors are heavy and bulky not because that is desirable but because spaciousness is.

So we invented workarounds like standing desks, monitor stands, and ergonomic chairs. There is a huge market of contraptions trying to reinvent the human-computer relationship. People try some [truly creative ways](https://mgsloan.com/posts/deck-desk/) of making their computing more humane. But these are patchwork solutions. They are not addressing the core issue; we are accommodating the technology, not the other way around.

Lack of movement has consequences. [According to the WHO, insufficient physical activity is the 4th leading risk factor for mortality.](https://www.who.int/data/gho/indicator-metadata-registry/imr-details/3416) [Sitting has been called the "new smoking".](https://www.latimes.com/science/sciencenow/la-sci-sn-get-up-20140731-story.html) We lose muscle, stability, flexibility, and cardiovascular health. We develop back and neck problems from sitting too much, and sometimes also knee, foot, or ankle problems from standing too much. Often, these changes are gradual and go unnoticed until they become severe. As most people eventually find out firsthand, an ounce of prevention is worth a pound of cure.


### Finger Fatigue

As the saying goes, variety is the spice of life – and this holds true for movements as well. We are built for diverse movements, but computing forces us into monotonous unnatural movements.

The traditional input methods, primarily keyboards and mice or trackpads, confine us to a 2D plane. Repetitive movements in constrained positions lead to ailments like Repetitive Strain Injury (RSI) and Carpal Tunnel Syndrome (CTS). Our wrists and fingers weren't designed for the repetitive rigidity these devices demand. [Many famous programmers have RSI.](http://xahlee.info/emacs/emacs/emacs_hand_pain_celebrity.html)


### Myopia Menace

Our eyes need variety too. When focusing on our screens, our focal distance is very near and usually fixed. We rarely adjust the distance between the screen and our eyes. But we're evolved to focus at a variety of distances. By neglecting to do so, our eyes atrophy.

Staring at screens for extended periods, especially close-up, has been linked to an increase in [nearsightedness](https://www.mayoclinic.org/diseases-conditions/nearsightedness/symptoms-causes/syc-20375556). It's alarming that [myopia is expected to affect more than half the world's population by 2050](https://www.weforum.org/agenda/2022/11/short-sightedness-cases-rising-globally/).

## Personal Motivation

I suffer from chronic and mysterious lower back and pelvic pain. I have consulted many specialists and undergone many tests and scans, but none have been able to ease the pain or even give me a firm diagnosis.

One thing that reliably soothes the pain: movement. Sitting in a chair is the worst, but remaining in any fixed position for more than roughly 30 minutes (or little as 10 minutes on a bad day) will trigger significant discomfort or pain.

This condition has heightened my sensitivity to the physical toll that conventional computing imposes on the human body. While my situation may be more acute, prolonged unnatural behaviors wear down all bodies over time. The repercussions may not be immediate, but eventually, the human body is unable to overcome such impositions.

_(For any reader who might possess insights into chronic lumbar/sacral pain that improves with movement and is associated with colon fullness, I have elaborated on my condition in this footnote.[^chronic-pain] I'd be grateful for any leads that might steer me towards a diagnosis.)_

[^chronic-pain]: The onset of my chronic lumbar/sacral pain dates back to 2019. Manifesting as a constant, diffuse ache originating from the lower back or sacrum, the pain lessens with physical activity and after bowel movements. Extended periods of immobility, particularly sitting, as well as constipation, exacerbate the pain. I have sought opinions from multiple physical therapists (including pelvic floor specialists), a sports medicine doctor, an orthopedic surgeons, a gastroenterologist, multiple pain management doctors, a rheumatologist, and several general practitioners. I have undergone a lumbar X-ray, multiple MRIs of my lumbar, pelvis, and sacroilliac joints, a colonoscopy, extensive blood tests for autoimmune markers, and epidural steroid injections into L5-S1 (where I have a 2mm broad based disc protrusion). However, test results have been largely unremarkable, and no intervention has proven effective. Most recently, I had ultrasound of my SI joints, which unexpectedly found inflammation of the SI ligaments. Steroid injections into the SI joint area seem to have helped slightly, which is my best lead so far, so I am pursuing the hypothesis that all of this is somehow caused by SI joint ligament inflammation. If you have any suggestions, please contact me through the social media links on this website or via email at my firstName@thisDomain.

In an effort to keep the pain at bay, I maintain a routine of exercise and stretching. This is my small homage to my body that tolerates my sedentary work lifestyle. As a software engineer, my profession necessitates periods of focused, stationary work --- a harsh reality that I often ponder upon, especially in the context of my condition. I've been on a quest to discover methods to engage with technology without punishing my body.

### My Quest

Around 18 months ago, I bought a [Meta Quest 2](https://www.meta.com/quest/products/quest-2/) VR headset. The initial plan was to use it for gaming on my feet and staying active, but to my surprise, it turned out to be fantastic for cardio-style fitness gaming. It helped with my chronic pain and, fortunately, I didn't suffer from VR-induced nausea or dizziness, so that got me thinking: "Could I use this for work?" Turns out, not so much.

The primary issue was the insufficient resolution, especially for handling text. To comfortably peruse text, the human eye needs at least 40 pixels per degree (PPD). The Quest 2 sits at half that benchmark --- a mere 20 PPD.

The input lag was the second showstopper. While gaming mostly sidesteps this since games run on device, when you attempt to work with virtual displays connected to a nearby Mac, latency is considerable. This is exacerbated by the poor passthrough, which makes non-touch typing infeasible.

I also ran into issues with navigation. Hand tracking on Quest 2 is very laggy and imprecise. The controllers work far better, but, by picking them up, you forgo the ability to type. An external hardware keyboard is the only practical text input method available on Quest 2. The virtual keyboard is too slow and cumbersome and dictation is not viable.

I was excited to learn about the [Quest Pro](https://www.meta.com/quest/quest-pro/), but it has only about [22 PPD](https://www.uploadvr.com/meta-retinal-resolution-product-roadmap/) and, according to reviews, suffers from similar issues as the Quest 2, though to a lesser extent.

Perhaps a simple head-mounted display, instead of full VR, would work for me. I could be as mobile as a laptop allowed but avoid the ergonomic nightmare of physically joining the screen and keyboard. In that vein, I also tried an [XREAL Air](https://www.xreal.com/air/). It's an interesting device but the resolution is far too low, especially outside the center, for textual work.

To find an XR headset with low latency, high PPD, and good hand/eye tracking, one must look to the [Varjo XR-3](https://b2b-store.varjo.com/product/varjo-xr-3), which costs $6500 and also requires being tethered to a powerful Windows PC to run. I'm not even convinced it would create a humane computing environment, so that's a tough pill to swallow.

I was eagerly awaiting the rumored announcement of Apple's XR headset with the hope that it would solve many of my ergonomic issues while simultaneously not costing about $10,000 and requiring me to switch operating systems.

## A Thought Experiment

Let's engage in a thought exercise to conceptualize an optimal humane computing device, but limit our considerations to technology that is plausible within a 10-year time frame. No telepathic input or smell-o-vision output.

The device would support the user computing in a wide variety of body positions, not just sitting or standing. Laying down, squatting, or even yoga poses should be feasible. Further, the software would encourage physical movement and variety (within the user's capabilities), such that long-term extended use did not degrade the user's physical health.

The device would seamlessly support a variety of input methods, as necessitated by the user's diverse range of body positions. The software would guide the user towards the most ergonomic methods given the current body position but also ensure that the user doesn't rely on any one type of input too much, and thereby prevent repetitive use injuries. Input methods would not require external devices, but using external devices would be possible, especially when using enhanced sensory modalities like haptic feedback.

The screen(s) would be transparent (with UV protection), helping the user synchronize their circadian rhythm with nature. Lenses would be varifocal, allowing the user to focus on real and virtual objects at a variety of distances. Further, the software would encourage virtual objects and screens to be placed at a variety of distances, thereby encouraging the user to regularly change their focal distance and maintain healthy eyes.

Last, and perhaps without saying, the device would need to be practical for real computing and affordable.

## One more thing…
With that in mind, let's evaluate Apple's Vision Pro in the context of humane computing.

Though I was not fortunate enough to experience one of the private 30-minute demos at WWDC, my reflections are informed by reports from those demos, the WWDC keynote, WWDC [developer videos](https://developer.apple.com/wwdc23/topics/spatial-computing/), insights from Apple employees' dialogues in the relevant WWDC Slack channel, and playing with the Vision Pro simulator.

### Input Methods

The Vision Pro headset supports a variety of input methods, including gaze tracking, hand tracking, dictation, a virtual keyboard, and bluetooth devices like keyboards, mice/trackpads, and game controllers.

As an overall point, I love the variety of input methods. This on its own should go a long way to reducing RSI. Remember, our bodies and minds thrive on variety, so our computing devices should encourage that. Instead of confining input to a 2D plane, the Vision Pro seems to allow and encourage a variety of input methods.

#### Gaze Tracking

Demo reports have universally praised the eye tracking, some calling it borderline telepathic or magical.

Optimistically, it seems like Apple solved the cursor movement problem, obviating the need for a mouse/trackpad, regardless of body position. This seems like a huge win for humane computing. Of course, this may not work well for fine-grained cursor movement as might be needed in some domains like visual design.

#### Hand Tracking

Hand gestures are used for tapping, rotating, and various other common but lightweight actions. [This video](https://developer.apple.com/videos/play/wwdc2023/10073/) is helpful for understanding the default input methods.

![visionOS Hand Gestures](gestures.jpg "visionOS Hand Gestures")

Reports have also been very positive about this experience. Generally, I'm optimistic on this as well, but I see a few likely issues. For one, the finger detection is done using downward and forward looking sensors, which means that the user's hands need to be in view. This could be problematic both due to occlusion (the hand is in front of the headset but blocked from view of the headset due to unusual body position) or due to the user's hands being entirely out of view (like above their head).

Further, this precludes any body positions which uses your hands. Many yoga poses, or just hanging from a bar, would be impossible. Yes, I'm being nitpicky. In general, these gestures seem fantastic and, especially when combined with gaze tracking, should do a great job of preventing RSI.

#### Typing

Typing is supported via dictation, a virtual keyboard, and bluetooth keyboards.

Apple's dictation has been notoriously behind state-of-the-art solutions for years. There is some indication that this may be changing. For example, though not widely reported, [Apple also announced the ability to customize on-device speech recognition](https://developer.apple.com/videos/play/wwdc2023/10101/), which may make dictation usable in non-prose contexts, such as coding. Don't hold your breath though. The likelihood is that dictation will be usable only for small lossy inputs, like search queries, and probably not very usable for long-form prose and completely unusable for coding.

I was hopeful for the virtual keyboard, but, if the Vision Pro simulator is any indication, it is a big disappointment for many tasks. It's essentially the iOS keyboard.

![visionOS Software Keyboard](software_keyboard.jpg "visionOS Software Keyboard in Simulator")


As with iOS, inputting numbers with it requires first tapping the `123` key to change the layout to numeric. Inputting many common coding symbols, like braces, requires an additional tap on the `#+=` key to then navigate to a third layout. Further, keyboard shortcuts are impossible with this layout. This is absurd. The visionOS virtual keyboard is not space-constrained like the iOS virtual keyboard. Perhaps they are going for familiarity or, more likely, there is some significant tech debt which makes it infeasible to ship a full virtual keyboard. Regardless, it's a huge disappointment.

Unfortunately, it seems like a bluetooth keyboard will not be an optional accessory if one wants to use the Vision Pro for real computing; it will be a required accessory. 

The use of an external hardware keyboard presents a problem for any body position which does not put the hands in a natural position for typing on a horizontal surface, which severely limits potential poses.

Still, this is much better than our current paradigm on laptops and desktops. Much computing can be achieved without any external input and, for the rest, a bluetooth keyboard is much less restrictive than a laptop or desktop setup.

The bigger issue for humane computing is that both the virtual keyboard and physical keyboard retain the 2D typing interface, which is prone to causing issues. In 3D, we should be able to input text faster, more accurately, and with less repetitive motion. I imagine some sort of wearable [chorded keyboard](https://en.wikipedia.org/wiki/Chorded_keyboard). Or even advanced gesture recognition could be used to achieve this. The learning curve would be steep but the payoff would be huge.

I'm keeping my eyes on the [TapXR](https://www.tapwithus.com/). I suspect it won't function well for long-form prose or coding, but I hope I'm wrong.

I wish Apple really reinvented typing for spatial computing, even if the learning curve was significant. They're likely being conservative and trying to make the transition as easy as possible for their userbase, but I think they're missing an opportunity to make a big leap forward.

### Movement and Versatile Body Positions

Disappointingly, virtually all of Apple's pitch for the Vision Pro showed people sitting or standing stationary.

![Stationary Collage](stationary_collage.jpg "Sit or stand while using Vision Pro")

But the Vision Pro doesn't have to be used the way Apple pitched it. We don't need to be running while computing, but we need to be able to routinely change our body position without significantly negatively impacting productivity. It would have been great to see people working while laying on their back, or while in a primal squat, or really in any of the myriad of natural body positions which are not currently well accommodated by our computing technology. Based on the available public information, this failure to demonstrate the Vision Pro's potential for versatile ergonomic positions is a missed opportunity, but likely a marketing decision, not a functional limitation of the device.

One significant caveat: it seems like windows in visionOS are always positionally fixed in space or anchored to physical reality. In other words, it doesn't seem possible to arrange windows and then have them automatically rotate to face you as your head moves. This might make it difficult to frequently reposition from sitting to standing to squatting to lounging as, each time, you may need to manually reposition windows to be optimally viewable.

Of course, we can't forget that the headset is tethered to an external battery and that battery lasts a mere two hours, so, for extended use, you'll need to be near a power outlet. Further, the connector to the headset is not magnetic; it's appears to lock on via partial rotation. This means snagging the cable could be dangerous because it could cause the head to jerk to the side. Essentially, it's designed for stationary use. I suspect third-party headbands will situate the battery pack at the back of the head (to act as a counterweight) and provide larger or hot-swappable batteries.

Additionally, the Vision Pro might introduce novel ergonomic issues, like neck strain, but I'm optimistic that it will be a net positive as it will enable increased mobility.

### What about actual vision?

> ["It's the first Apple product you look through and not at."](https://www.youtube.com/live/GYkq9Rgoj8E?feature=share&t=4962) -- Tim Cook, WWDC23 keynote

Technically, you look through it in the same way you look through an iPhone camera viewfinder, but it's a good line. And it may actually be very important.

The Vision Pro, based on reports of those who have used it, seems to actually be usable for work. At over 4k resolution per eye, the Vision Pro finally seems like a device that can be used for real textual work. Hallelujah!

But what about myopia? One might think that having screens mere inches away from the eyes would be worse than the status quo. However, the eye health issue is related to focal distance, not screen distance --- a crucial distinction.

In VR, the focal distance can be much further than traditional screens. Additionally, virtual screens can be independently positioned along the Z axis, potentially leading to healthy variation in focal distance. So since the VR screens are perceptually further than screens on a laptop or desktop, they should be better for preventing myopia? The answer is complicated.

The issue is [Vergence-accommodation conflict](https://en.m.wikipedia.org/wiki/Vergence-accommodation_conflict) (VAC). Essentially, there are two kinds of focus. One is vergence. If you imagine lasers coming out of your eyes like Superman, vergence is where the two beams converge. The other is accommodation, which is focus by changing optical power of the eye using tiny muscles. In the real world, vergence and accommodation synchronize. But not in VR, where accommodation is fixed but vergence varies.

![Vergence-Accommodation Conflict Diagram](Vergence-Accommodation_Conflict_Diagram.png "Vergence-Accommodation Conflict Diagram (credit: Wikipedia)")

In Vision Pro, accommodation is set at a fixed distance of about 2 meters. Additionally, Apple is trying to keep most of the vergence distance at near the fixed accommodation distance via default window placement and design guidelines, though it's not clear how well this will work in practice. But, the status quo is pretty bad, being mostly fixed vergence and accommodation at a distance of about 0.5 meters. So the decision is between slightly variable near focal distance or partially fixed partially variable medium focal distance. 
 
It's not clear yet which is better for long-term eye health. It's possible that Vision Pro VAC will be better for eye health than the status quo. It's also possible that it could be disastrous. There don't seem to be any long-term studies on VAC and adult eye health. Perhaps this is because there hasn't been a consumer VR product which is actually usable for extended periods. The Vision Pro will hopefully lead to more research on this topic.

Eventually, we need varifocal optics, which will resolve VAC by allowing natural synchronization of vergence and accommodation. For now, this is the biggest open question for the Vision Pro and humane computing. For my personal case, where I am significantly less concerned about eye health than other physical health, this is a tradeoff worth making. But for those inversely inclined, consider the potential long-term risks of both the Vision Pro and the status quo.

Additionally, Vision Pro unfortunately likely does nothing to help with the [issues caused by extended blue light exposure](https://health.ucdavis.edu/blog/cultivating-health/blue-light-effects-on-your-eyes-sleep-and-health/2022/08) from screens. To hazard a guess, I would think it would substantially increase blue light exposure.

### Practicality

All of this is irrelevant if the Vision Pro is not practical for real computing use. To date, only Apple's legacy platform, MacOS, is usable for real computing work. Apple pitches iPadOS on iPad Pro as a "computer replacement", but it's not. It's a consumption device. It's a great consumption device, but it's not a computer replacement, unfortunately. The hardware is definitely there, but the software is not. Apple does not allow software compilation on non-MacOS devices, which makes most types of software development impossible or impractical. Even compiling code for Apple's own devices using Xcode is not supported on iPadOS. Further, even types of computing which do not inherently run afoul of Apple's arbitrary restrictions are hindered by the lack of a proper filesystem and the inability to run background processes. For example, Apple recently released Final Cut Pro for iPad, but, comically, you need to keep the app open while exporting because iPadOS will kill long-running processes in backgrounded apps.

Unfortunately, all things point to the Vision Pro being somewhat akin to an iPad Pro strapped to your face. It's clear that visionOS is derivative of iPadOS more so than any other Apple OS. Multitasking on visionOS looks suspiciously like a 3D version of Stage Manager on iPadOS. To some degree, even the backgrounded app limitation noted above will likely apply in visionOS because visionOS considers open windows which the user has not recently looked at to be deprioritized.

We can hope that the EU's Digital Markets Act (DMA) and similar regulation from other governments forces Apple to begrudgingly make their devices more usable for general computing. However, to some degree, it will always be necessary to offload heavy compute to external devices. Our computation demands grow in lockstep with our computation ability. A mediocre consumer laptop today would blow a top-of-the-line consumer desktop from 30 years ago out of the water. Even if Apple eventually manages to make a very lightweight Vision product with computation power exceeding that of an M2 Ultra (Apple's current top-of-the-line desktop chip), by that time the most demanding computation tasks will require power far exceeding that of an M2 Ultra. I believe this is somewhat related to why Apple shipped an external battery pack instead of incorporating it into the rear of the headset; they are preparing consumers for a future where compute is offloaded to a wired device. 

All of this is to say that there is nothing inherently wrong with offloading computation to external devices. Unfortunately, Apple's initial implementation of this is very limited. In visionOS 1.0, only a single Mac screen can be mirrored. The ideal case would be running native applications on an open headset (see DMA hope above) where heavy compute is transparently offloaded to external devices (instead of just mirroring an external device display). This is a far cry from what we'll see in Vision Pro next year, but it's a potential future scenario.

## The Vision

When I began writing this post, I was beyond excited about the impending humane computing solution that is the Vision Pro. But, in the process of writing and researching, my expectations have been tampered significantly. I still think the Vision Pro is a big step in the right direction, but it's marketed and designed without humane computing at the forefront. But here's the good news: Apple doesn't yet know what Vision is for.

When Apple Watch launched, Apple marketed it as a fashion accessory with social features. They sold 18-karat gold versions. They shipped with [Digital Touch, a weird feature that allowed Apple Watch users to send animations of their heart beat and little drawings](https://www.macworld.com/article/671630/how-to-use-digital-touch-on-apple-watch.html). Interestingly, in the linked article, Macworld called Digital Touch "one of [the Apple Watch's] key features". This feature was so emphasized that it had hardware support; pressing the side button opened your "Friends" list, a selection screen of up to 12 contacts from which you could send a Digital Touch. Today, I'd wager few even know Digital Touch still exists as an iMessage app. (In fact, I rediscovered it while writing this post.)

But developers and users, and then Apple, figured out that Apple Watch was actually a health and fitness device with some other occasionally useful features (like telling time). Quickly, Apple shifted their marketing and development focus to health and fitness. Every year they add more health and fitness features to watchOS, and almost every year they add hardware sensors for health monitoring. 

History is repeating itself with the Vision. Apple's marketing of the Vision Pro is all over the place. It's for entertainment. It's for productivity. It's for collaboration. It's for social situations and capturing 3D imagery. It's for (very limited) gaming. Unfortunately, the commonality between these uses in Apple's imagery is that users are relatively sedentary and sitting or standing. But Apple doesn't know what Vision will be yet. They have lots of ideas, but ultimately the market will figure it out and Apple with seamlessly shift to that without ever admitting prior ignorance. 

I think Vision is for humane computing. I *hope* Vision is for humane computing. The important takeaway is that *we get to choose*. We, particularly developers and enthusiasts, get to decide what Vision is for. Apple will follow.

They pitched some ideas in the keynote, but the market will decide what it's for. I think it will be for humane computing. Devices will get progressively more ergonomic and versatile. It is also within our power, as users and developers, to guide and demand this evolution. We should demand that our computing devices do not harm us. We should demand that they enable us to act within our nature and compute simultaneously. We should demand humane computing.


Spatial computing is a distraction from the more important shift. Apple Vision Pro will hopefully prove to be the vanguard of the humane computing era.

Disclosure: I'm long AAPL.

